"""
PyTorch Dataset and DataLoader for PCB Thermal Data

Handles loading and preprocessing of generated thermal datasets.
"""

import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
from typing import Tuple, Optional, Dict, Callable
import json


class PCBThermalDataset(Dataset):
    """
    PyTorch Dataset for PCB thermal prediction.
    
    Loads data from .npz files generated by dataset_generator.py
    
    Input channels (4):
        0: Copper density (0-1)
        1: Via map (0-1)
        2: Component footprints (0-1)
        3: Power map (normalized)
        
    Output (1):
        Temperature field (°C)
    """
    
    def __init__(
        self,
        data_path: str,
        split: str = "train",
        transform: Optional[Callable] = None,
        target_transform: Optional[Callable] = None,
        normalize_output: bool = True,
        output_stats: Optional[Dict] = None
    ):
        """
        Args:
            data_path: Path to directory containing .npz files
            split: One of "train", "val", "test"
            transform: Optional transform for inputs
            target_transform: Optional transform for outputs
            normalize_output: Whether to normalize temperature output
            output_stats: Pre-computed mean/std for normalization
        """
        self.data_path = Path(data_path)
        self.split = split
        self.transform = transform
        self.target_transform = target_transform
        self.normalize_output = normalize_output
        
        # Load data
        npz_file = self.data_path / f"{split}.npz"
        if not npz_file.exists():
            raise FileNotFoundError(f"Data file not found: {npz_file}")
            
        data = np.load(npz_file)
        self.inputs = data['inputs']   # (N, 4, H, W)
        self.outputs = data['outputs'] # (N, 1, H, W)
        
        # Compute or use provided stats for normalization
        if normalize_output:
            if output_stats is not None:
                self.output_mean = output_stats['mean']
                self.output_std = output_stats['std']
            else:
                self.output_mean = self.outputs.mean()
                self.output_std = self.outputs.std()
        else:
            self.output_mean = 0
            self.output_std = 1
            
        # Load metadata if available
        metadata_file = self.data_path / "metadata.json"
        if metadata_file.exists():
            with open(metadata_file, 'r') as f:
                all_metadata = json.load(f)
            # Filter to this split (approximate - metadata includes all samples)
            self.metadata = all_metadata[:len(self.inputs)]
        else:
            self.metadata = None
            
    def __len__(self) -> int:
        return len(self.inputs)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Returns:
            inputs: (4, H, W) tensor of PCB features
            outputs: (1, H, W) tensor of temperature (optionally normalized)
        """
        inputs = self.inputs[idx].astype(np.float32)
        outputs = self.outputs[idx].astype(np.float32)
        
        # Normalize output
        if self.normalize_output:
            outputs = (outputs - self.output_mean) / (self.output_std + 1e-8)
            
        # Convert to tensors
        inputs = torch.from_numpy(inputs)
        outputs = torch.from_numpy(outputs)
        
        # Apply transforms
        if self.transform:
            inputs = self.transform(inputs)
        if self.target_transform:
            outputs = self.target_transform(outputs)
            
        return inputs, outputs
    
    def get_output_stats(self) -> Dict[str, float]:
        """Return normalization statistics"""
        return {
            'mean': float(self.output_mean),
            'std': float(self.output_std)
        }
    
    def denormalize(self, normalized_output: torch.Tensor) -> torch.Tensor:
        """Convert normalized output back to temperature in °C"""
        return normalized_output * self.output_std + self.output_mean


class RandomFlip:
    """Random horizontal and vertical flip augmentation"""
    
    def __init__(self, p: float = 0.5):
        self.p = p
        
    def __call__(self, x: torch.Tensor) -> torch.Tensor:
        if torch.rand(1) < self.p:
            x = torch.flip(x, dims=[-1])  # Horizontal
        if torch.rand(1) < self.p:
            x = torch.flip(x, dims=[-2])  # Vertical
        return x


class RandomRotate90:
    """Random 90-degree rotation augmentation"""
    
    def __init__(self, p: float = 0.5):
        self.p = p
        
    def __call__(self, x: torch.Tensor) -> torch.Tensor:
        if torch.rand(1) < self.p:
            k = torch.randint(1, 4, (1,)).item()  # 1, 2, or 3 rotations
            x = torch.rot90(x, k, dims=[-2, -1])
        return x


class AddNoise:
    """Add Gaussian noise to inputs"""
    
    def __init__(self, std: float = 0.02):
        self.std = std
        
    def __call__(self, x: torch.Tensor) -> torch.Tensor:
        return x + torch.randn_like(x) * self.std


def get_dataloaders(
    data_path: str,
    batch_size: int = 16,
    num_workers: int = 4,
    augment: bool = True,
    normalize: bool = True
) -> Tuple[DataLoader, DataLoader, DataLoader, Dict]:
    """
    Create train, val, test dataloaders.
    
    Args:
        data_path: Path to dataset directory
        batch_size: Batch size
        num_workers: Number of data loading workers
        augment: Whether to use data augmentation on training
        normalize: Whether to normalize outputs
        
    Returns:
        train_loader, val_loader, test_loader, output_stats
    """
    # Transforms
    train_transform = None
    if augment:
        # Simple augmentation that works for both input and output
        pass  # For thermal prediction, we'll handle augmentation differently
    
    # Create datasets
    train_dataset = PCBThermalDataset(
        data_path, 
        split="train",
        normalize_output=normalize
    )
    
    # Use training stats for val/test
    output_stats = train_dataset.get_output_stats()
    
    val_dataset = PCBThermalDataset(
        data_path,
        split="val",
        normalize_output=normalize,
        output_stats=output_stats
    )
    
    test_dataset = PCBThermalDataset(
        data_path,
        split="test",
        normalize_output=normalize,
        output_stats=output_stats
    )
    
    # Create dataloaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )
    
    return train_loader, val_loader, test_loader, output_stats


def test_dataloader():
    """Test dataloader with sample data"""
    import sys
    from pathlib import Path
    
    data_path = Path(__file__).parent.parent.parent / "data" / "synthetic"
    
    if not data_path.exists():
        print(f"Test data not found at {data_path}")
        print("Run 'python scripts/generate_dataset.py --quick' first")
        return
        
    print(f"Loading data from {data_path}...")
    
    train_loader, val_loader, test_loader, stats = get_dataloaders(
        str(data_path),
        batch_size=4,
        num_workers=0
    )
    
    print(f"\nDataset sizes:")
    print(f"  Train: {len(train_loader.dataset)}")
    print(f"  Val:   {len(val_loader.dataset)}")
    print(f"  Test:  {len(test_loader.dataset)}")
    
    print(f"\nOutput normalization stats:")
    print(f"  Mean: {stats['mean']:.2f}°C")
    print(f"  Std:  {stats['std']:.2f}°C")
    
    # Test batch
    inputs, outputs = next(iter(train_loader))
    print(f"\nBatch shapes:")
    print(f"  Inputs:  {inputs.shape}")
    print(f"  Outputs: {outputs.shape}")
    
    print(f"\nInput ranges:")
    print(f"  Copper:     [{inputs[:, 0].min():.3f}, {inputs[:, 0].max():.3f}]")
    print(f"  Vias:       [{inputs[:, 1].min():.3f}, {inputs[:, 1].max():.3f}]")
    print(f"  Components: [{inputs[:, 2].min():.3f}, {inputs[:, 2].max():.3f}]")
    print(f"  Power:      [{inputs[:, 3].min():.3f}, {inputs[:, 3].max():.3f}]")
    
    print(f"\nOutput range (normalized): [{outputs.min():.3f}, {outputs.max():.3f}]")
    
    # Denormalize
    denorm_outputs = train_loader.dataset.denormalize(outputs)
    print(f"Output range (°C):         [{denorm_outputs.min():.1f}, {denorm_outputs.max():.1f}]")


if __name__ == "__main__":
    test_dataloader()
